{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blR8hhA_e-4S"
      },
      "source": [
        "# LongLLaMA Code: Focused Transformer Training for Context Scaling\n",
        "**LongLLaMA-Code-Instruct-7B is a LLM that builds upon the foundation of CodeLLaMA using FoT context extension and instruction tuning**.\n",
        "\n",
        "It is created by first taking the [CodeLLaMA-7B](https://huggingface.co/codellama/CodeLlama-7b-hf), continuing pre-training with [Focused Transformer (FoT)](https://arxiv.org/abs/2307.03170) method, and then instruction tuning the model.\n",
        "\n",
        "This notebook is a demo of [LongLLaMA-Code-Instruct-7B](TODO).  \n",
        "To create the model we have used the following datasets:\n",
        "* [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n",
        "* [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n",
        "* [ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)\n",
        "\n",
        "For more, see the [FoT paper](https://arxiv.org/abs/2307.03170) and the [GitHub repository](https://github.com/CStanKonrad/long_llama).  \n",
        "Note that as we have started from the Meta's [CodeLLaMA-7B](https://huggingface.co/codellama/CodeLlama-7b-hf) model and used GPT outputs to finetune it, the model is aimed for research purposes only.\n",
        "\n",
        "\n",
        "**We provide the basic code for quantization that should suffice to run most of the demo parts on the free Colab GPU.**  \n",
        "**However, the quantization code is not optimized and may result in reduced performance.**  \n",
        "**Generation with free colab GPU may be slow, but processing the long input should take 1-2 min**  \n",
        "**Running this model without quantization may require an A100 40GB GPU.**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTROtgpafB_8"
      },
      "source": [
        "# Initial steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La3cPHfCe7hU",
        "outputId": "a4b15130-cef6-432f-f7c2-b6e3fe5f9b2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "Collecting transformers==4.33.2\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m791.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, transformers, nvidia-cusolver-cu12, accelerate\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "Successfully installed accelerate-0.28.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 tokenizers-0.13.3 transformers-4.33.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers==4.33.2 sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x_utNYxfECA",
        "outputId": "76fe7668-4ace-4092-a1c4-5416a70c91ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, PreTrainedModel, PreTrainedTokenizer\n",
        "from typing import List, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrrFlXKMfHMs",
        "outputId": "cb52bc06-21ca-46a9-b77c-511490a3e7bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "MODEL_PATH = \"syzymon/long_llama_code_7b_instruct\"\n",
        "TOKENIZER_PATH = MODEL_PATH\n",
        "# to reduce GPU memory usage we will use reduced precision\n",
        "TORCH_DTYPE = torch.bfloat16\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFQ5Qr8DI7Fz"
      },
      "outputs": [],
      "source": [
        "# To fit most of the demo parts on a single Google Colab GPU we\n",
        "# provide a basic unoptimized quantization code\n",
        "# change to False to disable the quantization\n",
        "QUANTIZED = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eec2495bb83f4e4eafcfc4a3e819c5dc",
            "996097452ccc47cfbb63a826c7ecfa37"
          ]
        },
        "id": "5fE5Z1ABfJUp",
        "outputId": "b4ddc1c8-e34c-49e4-9141-11bc384bb50f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!!!WARNING!!!!! The mode will be quantized to 8 bits!\n",
            "This may affect the model performance!\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mfatal: destination path 'long_llama' already exists and is not an empty directory.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eec2495bb83f4e4eafcfc4a3e819c5dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "996097452ccc47cfbb63a826c7ecfa37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model_8bit.bin:   0%|          | 0.00/7.01G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
        "\n",
        "# unoptimized quantization code for running with free Colab GPU\n",
        "def load_and_qunatize_model(num_bit: int, model_path):\n",
        "    print(f\"!!!!!WARNING!!!!! The mode will be quantized to {num_bit} bits!\\n\"\n",
        "          \"This may affect the model performance!\")\n",
        "\n",
        "    !pip3 install huggingface_hub\n",
        "    !pip3 install bitsandbytes\n",
        "    !git clone https://github.com/CStanKonrad/long_llama.git\n",
        "    !cp -r long_llama/src long_llama_code/\n",
        "    from long_llama_code.modeling_longllama import LongLlamaForCausalLM\n",
        "    from long_llama_code.configuration_longllama import LongLlamaConfig\n",
        "    from transformers import AutoConfig\n",
        "    from accelerate.utils import BnbQuantizationConfig\n",
        "    from accelerate.utils import load_and_quantize_model\n",
        "    from accelerate import init_empty_weights\n",
        "    from huggingface_hub import snapshot_download, hf_hub_download\n",
        "\n",
        "\n",
        "    cfg = LongLlamaConfig.from_pretrained(model_path)\n",
        "    cfg.mem_attention_grouping = (1, 1024)\n",
        "    with init_empty_weights():\n",
        "        empty_model = LongLlamaForCausalLM(cfg)\n",
        "\n",
        "    gc.collect()\n",
        "    if num_bit == 8:\n",
        "        weights_loc = hf_hub_download(repo_id=MODEL_PATH, filename=\"quantized/pytorch_model_8bit.bin\")\n",
        "        bnb_quantization_config = BnbQuantizationConfig(load_in_8bit=True, llm_int8_threshold = 6)\n",
        "    elif num_bit == 4:\n",
        "        # May give out of RAM on Colab\n",
        "        weights_loc = snapshot_download(MODEL_PATH) #MODEL_PATH\n",
        "        bnb_quantization_config = BnbQuantizationConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "    else:\n",
        "        raise ValueError(f\"{num_bit} quantization not supported.\")\n",
        "\n",
        "    gc.collect()\n",
        "    model = load_and_quantize_model(empty_model, weights_location=weights_loc, bnb_quantization_config=bnb_quantization_config, device_map=\"auto\")\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "if not QUANTIZED:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_PATH,\n",
        "        torch_dtype=TORCH_DTYPE,\n",
        "        device_map=device,\n",
        "        trust_remote_code=True,\n",
        "        # mem_attention_grouping is used\n",
        "        # to trade speed for memory usage\n",
        "        # for details, see the section Additional configuration\n",
        "        # in the Github repository\n",
        "        mem_attention_grouping=(1, 1024),\n",
        "    )\n",
        "    model.eval()\n",
        "else:\n",
        "    model = load_and_qunatize_model(8, MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElblcG7MfOM4"
      },
      "source": [
        "# The demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chPK2r8CfRcC"
      },
      "source": [
        "## Question answering on long documents\n",
        "Here we show the ability of the model to answer questions about long documents.   \n",
        "As it is a 7B parameter model it should be better than the [3B parameter one](https://colab.research.google.com/github/CStanKonrad/long_llama/blob/main/long_llama_instruct_colab.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkOC3vsgI7F3"
      },
      "source": [
        "### Downloading and memory loading\n",
        "Code for downloading files and loading them into model memory.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt0PMfKDfLjy"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "def get_file(url: str, main_file: Optional[str] = None):\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        file_path = os.path.join(tmp_dir, \"_paper.tmp\")\n",
        "        if main_file is not None:\n",
        "            # we are dealing with an archive\n",
        "            file_path += \".tar\"\n",
        "\n",
        "        urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "        if main_file is not None:\n",
        "            # we are dealing with an archive\n",
        "            shutil.unpack_archive(file_path, tmp_dir)\n",
        "            main_file_path = os.path.join(tmp_dir, main_file)\n",
        "        else:\n",
        "            main_file_path = file_path\n",
        "\n",
        "        with open(main_file_path, \"r\") as f:\n",
        "            data = f.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_paper(url: str, main_file: Optional[str] = None):\n",
        "    return get_file(url, main_file)\n",
        "\n",
        "\n",
        "def get_files(url_list: List[str]):\n",
        "    data = []\n",
        "    for url in url_list:\n",
        "        data.append(get_file(url))\n",
        "\n",
        "    data = \"\\n\".join(data)\n",
        "    return data\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def load_to_memory(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, text: str):\n",
        "    tokenized_data = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_data.input_ids\n",
        "    input_ids = input_ids.to(model.device)\n",
        "    torch.manual_seed(0)\n",
        "    output = model(input_ids=input_ids)\n",
        "    memory = output.past_key_values\n",
        "    return memory\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with_memory(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, memory, prompt: str, temperature=0.2):\n",
        "    tokenized_data = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_data.input_ids\n",
        "    input_ids = input_ids.to(model.device)\n",
        "\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
        "\n",
        "    new_memory = memory\n",
        "\n",
        "    stop = False\n",
        "    while not stop:\n",
        "        output = model(input_ids, past_key_values=new_memory, last_context_length=3072)\n",
        "        new_memory = output.past_key_values\n",
        "        assert len(output.logits.shape) == 3\n",
        "        assert output.logits.shape[0] == 1\n",
        "        last_logit = output.logits[[0], [-1], :]\n",
        "        dist = torch.distributions.Categorical(logits=last_logit / temperature)\n",
        "        next_token = dist.sample()\n",
        "        if next_token[0] == tokenizer.eos_token_id:\n",
        "            streamer.put(next_token[None, :])\n",
        "            streamer.end()\n",
        "            stop = True\n",
        "        else:\n",
        "            input_ids = next_token[None, :]\n",
        "            streamer.put(input_ids)\n",
        "\n",
        "\n",
        "PROMPT_PREFIX = \"You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can.\\n\\n\"\n",
        "\n",
        "\n",
        "def construct_question_prompt(question: str):\n",
        "    prompt = (\n",
        "        f\"\\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can.\\n\\n\"\n",
        "        \"Answer the question below using the information from the text above.\\n\"\n",
        "        f\"Question: {question}\\nAnswer: \"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def ask_model(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prompt: str, memory, seed=0):\n",
        "    tokenized_data = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_data.input_ids\n",
        "    input_ids = input_ids.to(model.device)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    generate_with_memory(model, tokenizer, memory, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz6rOAqaI7F5"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del chatbot\n",
        "except:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHfWdyCdI7F6"
      },
      "source": [
        "### Questions about code\n",
        "We download the instruction tuning files from the long_llama repository and ask the model questions about the implementation.\n",
        "Each question is asked independently without updating the memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9o4XDA5I7F6"
      },
      "outputs": [],
      "source": [
        "instruct_dp = get_files(\n",
        "    [\n",
        "        \"https://raw.githubusercontent.com/CStanKonrad/long_llama/2c88620d0ec9c28e13b4c208be34ebac68b90e37/instruction_fine_tuning/arguments.py\",\n",
        "        \"https://raw.githubusercontent.com/CStanKonrad/long_llama/2c88620d0ec9c28e13b4c208be34ebac68b90e37/instruction_fine_tuning/data_processing.py\",\n",
        "        \"https://raw.githubusercontent.com/CStanKonrad/long_llama/2c88620d0ec9c28e13b4c208be34ebac68b90e37/instruction_fine_tuning/fine_tuning.py\",\n",
        "    ]\n",
        ")\n",
        "try:\n",
        "    del fot_memory\n",
        "except:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "fot_memory = load_to_memory(model, tokenizer, PROMPT_PREFIX + instruct_dp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITpGsFhgI7F7",
        "outputId": "eed2b619-a60c-4159-b15f-ae0cc3b48d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The purpose of this code is to train a LongLlamaForCausalLM model using the MixedTuneDataset and save the trained model and its parameters to a specified output directory.</s>\n"
          ]
        }
      ],
      "source": [
        "prompt = construct_question_prompt(\"What is the purpose of this code?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOyIC5zfI7F7",
        "outputId": "8d5ce2fa-fead-4e9e-b358-77c415b29e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most important functions and classes used for preparing the data are:\n",
            "\n",
            "- Data args: This dataclass contains the arguments related to the data processing, such as the data type, data filter, and data path.\n",
            "- Tokenization args: This dataclass contains the arguments related to the tokenization process, such as the pre-prompt text, the prompt field, the post-prompt text, and the post-response text.\n",
            "- Data processor: This class is used for preparing the data, tokenizing the text, and padding the data.\n",
            "- Data collator: This class is used for collating the data for training and evaluation.\n",
            "- MixedTuneDataset: This class is used for creating a dataset for training and evaluation.\n",
            "\n",
            "These classes and functions are used for preparing the data for training and evaluation.</s>\n"
          ]
        }
      ],
      "source": [
        "prompt = construct_question_prompt(\"What is used for preparing the data? Name the most important functions and classes.\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9urNlOSII7F8"
      },
      "outputs": [],
      "source": [
        "prompt = construct_question_prompt(\"Can you say something more about `tokenize_text_no_special_tokens`?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlvxxUTxI7F8"
      },
      "outputs": [],
      "source": [
        "prompt = construct_question_prompt(\"Can you say something more about `MixedTuneDataset`?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0wuD5qCI7F9"
      },
      "outputs": [],
      "source": [
        "prompt = construct_question_prompt(\"What are the main model configuration options? Enumerate them, use `` for their names.\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VZic5V_I7F9"
      },
      "outputs": [],
      "source": [
        "prompt = construct_question_prompt(\"What are the options to configure the data? Enumerate them, use `` for their names.\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmtswZ2II7F-"
      },
      "source": [
        "### Questions about FoT paper\n",
        "We download the FoT paper and ask basic questions about it's content.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14ao1rk6faHM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del fot_memory\n",
        "except:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "fot_paper = get_paper(url=\"https://raw.githubusercontent.com/CStanKonrad/long_llama/main/assets/fot_paper.tar\", main_file=\"fot_paper.tex\")\n",
        "if QUANTIZED:\n",
        "    fot_paper = fot_paper[:50000]\n",
        "fot_memory = load_to_memory(model, tokenizer, PROMPT_PREFIX + fot_paper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz8Gja1Nfdkb",
        "outputId": "dd010e97-9d2b-4dd2-d103-74e8307864ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The paper above is about the Focused Transformer (\\method{}) technique, which is a method for improving the context length in transformer models by endowing attention layers with access to an external memory. The paper discusses the design choices and ablations of the \\method{} technique, as well as its effectiveness in handling distractions and extrapolating to longer contexts.</s>\n"
          ]
        }
      ],
      "source": [
        "prompt = construct_question_prompt(\"What is the paper above about?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tQfPSURfejw"
      },
      "outputs": [],
      "source": [
        "prompt = construct_question_prompt(\"What method is introduced in the paper?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0xg6aBAfgDG",
        "outputId": "1988fd32-0e8b-4ded-a75e-72efa9069a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 3B model is called \\largeModels{} in the text.</s>\n"
          ]
        }
      ],
      "source": [
        "prompt = construct_question_prompt(\"How is the 3B model called by the authors?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd3wPEzAfhFF"
      },
      "outputs": [],
      "source": [
        "prompt = construct_question_prompt(\"Name all six authors of the presented paper.\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-mDT8TufjCl"
      },
      "outputs": [],
      "source": [
        "prompt = construct_question_prompt(\"What is the distraction issue?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugO81qgyI7GA",
        "outputId": "faeeb82d-045f-4ce3-e30d-0a96fd1a49e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The three main contributions of the paper are:\n",
            "1. Identifying the \\problem{} in long-context language models and proposing the \\method{} technique to address it.\n",
            "2. Demonstrating the effectiveness of \\method{} in improving the context length of existing models and fine-tuning pre-trained models.\n",
            "3. Providing ablation studies and additional analysis to support the effectiveness of \\method{} in various scenarios.</s>\n"
          ]
        }
      ],
      "source": [
        "prompt = construct_question_prompt('What are the three main contributions of the paper?')\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2qRVvyHI7GA"
      },
      "source": [
        "## Working with code\n",
        "\n",
        "The base [CodeLLaMA-7B](https://huggingface.co/codellama/CodeLlama-7b-hf) model was trained on a large amount of code data.    \n",
        "During the FoT tuning Python constituted a significant portion of the mixture.  \n",
        "For instruction tuning, we have utilized the [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct) dataset that contains both Chain of Thought and Python of Thought examples.  \n",
        "Because of all of the above, the model can be used for manipulation of the coded data that includes but is not limited to\n",
        "* refactoring\n",
        "* rewriting to other languages\n",
        "* explaining  \n",
        "\n",
        "However, as this model has only 7B parameters it can still make simple mistakes.  \n",
        "Below we show some examples of how the model can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaYJRNcrI7GB"
      },
      "source": [
        "### Code for the chat interface\n",
        "Here, we provide the code for communicating with the model in an iterative way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WteXHv6KI7GB"
      },
      "outputs": [],
      "source": [
        "class ChatOutputBuffer:\n",
        "    \"\"\"\n",
        "    For providing online output that\n",
        "    is truncated after generating specified (stop_text)\n",
        "    sequence of characters\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stop_text: List[str], tokenizer: PreTrainedModel):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
        "        self.max_stop_seq = 0\n",
        "        self.stop_seq = []\n",
        "        for st in stop_text:\n",
        "            self.stop_seq.append(st)\n",
        "            self.max_stop_seq = max(self.max_stop_seq, len(st))\n",
        "\n",
        "        self.output_buffer = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "    def reset_output_buffer(self):\n",
        "        self.output_buffer = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "    def advance_output(self):\n",
        "        beg = 0\n",
        "        end = len(self.output_buffer) - self.max_stop_seq\n",
        "\n",
        "        if end > beg:\n",
        "            output = self.output_buffer[beg:end]\n",
        "            self.streamer.put(output)\n",
        "            self.output_buffer = self.output_buffer[end:]\n",
        "\n",
        "    def flush_buffer(self):\n",
        "        if len(self.output_buffer) > 0:\n",
        "            self.streamer.put(self.output_buffer)\n",
        "            self.output_buffer = self.output_buffer[len(self.output_buffer) :]\n",
        "        self.streamer.end()\n",
        "\n",
        "    def generation_too_long(self, text: str) -> int:\n",
        "        end_requests = 0\n",
        "        for st in self.stop_seq:\n",
        "            if text.endswith(st):\n",
        "                end_requests += 1\n",
        "        return end_requests\n",
        "\n",
        "    def update_buffer(self, next_tok: int) -> bool:\n",
        "        assert isinstance(next_tok, int)\n",
        "\n",
        "        array_next_tok = np.array([next_tok], dtype=np.int64)\n",
        "        self.output_buffer = np.concatenate([self.output_buffer, array_next_tok], axis=0)\n",
        "\n",
        "        suffix = self.output_buffer[-self.max_stop_seq :]\n",
        "        decoded = self.tokenizer.decode(suffix)\n",
        "        end_requests = self.generation_too_long(decoded)\n",
        "        if end_requests > 0:\n",
        "            decoded = self.tokenizer.decode(suffix[1:])\n",
        "            while self.generation_too_long(decoded) == end_requests:\n",
        "                suffix = suffix[1:]\n",
        "                decoded = self.tokenizer.decode(suffix[1:])\n",
        "\n",
        "            left_intact = len(self.output_buffer) - len(suffix)\n",
        "\n",
        "            self.output_buffer = self.output_buffer[:left_intact]\n",
        "            self.flush_buffer()\n",
        "            return True\n",
        "\n",
        "        self.advance_output()\n",
        "        return False\n",
        "\n",
        "\n",
        "class SimpleChatBot:\n",
        "    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.prompt = \"A chat between a user (denoted as USER:) and an artificial intelligence assistant (denoted as ASSISTANT:). The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\\n\"\n",
        "        self.tokenized_prompt = self.tokenizer.encode(self.prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "        self.tokenized_prompt = torch.concatenate(\n",
        "            [torch.tensor([self.tokenizer.bos_token_id], dtype=torch.long).reshape(1, 1), self.tokenized_prompt],\n",
        "            dim=-1,\n",
        "        )\n",
        "        self.model_name = \"\\nASSISTANT: \"\n",
        "        self.tokenized_model_name = self.tokenizer.encode(\n",
        "            self.model_name, return_tensors=\"pt\", add_special_tokens=False\n",
        "        )\n",
        "        self.user_name = \"\\nUSER: \"\n",
        "        self.tokenized_user_name = self.tokenizer.encode(self.user_name, return_tensors=\"pt\", add_special_tokens=False)\n",
        "        self.past_key_values = None\n",
        "\n",
        "        self.t = 0.2\n",
        "        self.output_buffer = ChatOutputBuffer(\n",
        "            [self.model_name.strip(), self.user_name.strip(), self.tokenizer.eos_token], self.tokenizer\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ask(self, text: str):\n",
        "        input_ids = self.tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "        input_ids = torch.concatenate([self.tokenized_user_name, input_ids, self.tokenized_model_name], dim=-1)\n",
        "\n",
        "        if self.past_key_values is None:\n",
        "            input_ids = torch.concatenate([self.tokenized_prompt, input_ids], dim=-1)\n",
        "\n",
        "        self.output_buffer.reset_output_buffer()\n",
        "        output_text = self.model_name\n",
        "        output_ids = self.tokenizer.encode(\n",
        "            output_text, return_tensors=\"pt\", add_special_tokens=self.past_key_values is None\n",
        "        )\n",
        "        self.output_buffer.streamer.put(output_ids)\n",
        "\n",
        "        is_writing = True\n",
        "\n",
        "        step_id = 0\n",
        "\n",
        "        while is_writing:\n",
        "            input_ids = input_ids.to(model.device)\n",
        "            output = self.model(input_ids=input_ids, past_key_values=self.past_key_values)\n",
        "\n",
        "            logits = output.logits\n",
        "            assert len(logits.shape) == 3\n",
        "            assert logits.shape[0] == 1\n",
        "            last_logit = logits[[0], [-1], :]\n",
        "\n",
        "            if step_id <= 2:\n",
        "                last_logit[..., tokenizer.eos_token_id] = -1e4\n",
        "\n",
        "            dist = torch.distributions.Categorical(logits=last_logit / self.t)\n",
        "            next_token = dist.sample()\n",
        "            # Note that parts of cut out text may remain in model memory\n",
        "            # this is implemented in this way for performance reasons\n",
        "            past_key_values = output.past_key_values\n",
        "            assert len(next_token.shape) == 1\n",
        "            should_stop = self.output_buffer.update_buffer(next_token[0].cpu().item())\n",
        "            if should_stop:\n",
        "                is_writing = False\n",
        "            else:\n",
        "                input_ids = next_token[None, :]\n",
        "                self.past_key_values = past_key_values\n",
        "\n",
        "            step_id += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHJ5BErDI7GC"
      },
      "source": [
        "### Ask to improve/refactor/rewritte\n",
        "You can ask the model to improve the code.  \n",
        "Note that this is only a 7B parameter model, so the results may contain errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5VkKE2zI7GC",
        "outputId": "6b319913-f41e-4677-fbd1-5655313ae9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> \n",
            "ASSISTANT:  Sure, here's the code with more descriptive variable and function names:\n",
            "\n",
            "```c\n",
            "#include <bits/stdc++.h>\n",
            "\n",
            "const int MAX_INPUT_LENGTH = 2*1000000;\n",
            "int prefix_array[MAX_INPUT_LENGTH + 7];\n",
            "\n",
            "void compute_prefix_array(char *text, int length)\n",
            "{\n",
            "   prefix_array[0] = 0;\n",
            "   prefix_array[1] = 0;\n",
            "   int p = 0;\n",
            "   for (int i = 2; i <= length; ++i)\n",
            "   {\n",
            "       while (p != 0 && text[p + 1] != text[i])\n",
            "           p = prefix_array[p];\n",
            "       \n",
            "       if (text[p + 1] == text[i])\n",
            "           ++p;\n",
            "       \n",
            "       prefix_array[i] = p;\n",
            "   }\n",
            "}\n",
            "```\n",
            "In this code, the `compute_prefix_array` function takes a `text` pointer and a `length` integer as input, and it populates the `prefix_array` array with the prefix array values for the given text. The `prefix_array` array is used to store the prefix values for each character in the text. The `p` variable is used to keep track of the current position in the text, and it is updated based on the prefix values in the `prefix_array` array.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    del fot_memory\n",
        "except:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "chatbot = SimpleChatBot(model=model, tokenizer=tokenizer)\n",
        "chatbot.ask('''\n",
        "#include <bits/stdc++.h>\n",
        "\n",
        "const int MAX_INPUT_LENGTH = 2*1000000;\n",
        "int _preff[MAX_INPUT_LENGTH + 7];\n",
        "\n",
        "void _some_func(char *_text, int _dlugosc)\n",
        "{\n",
        "    _preff[0] = 0;\n",
        "    _preff[1] = 0;\n",
        "    int p = 0;\n",
        "    for (int i = 2; i <= _dlugosc; ++i)\n",
        "    {\n",
        "        while (p != 0 && _text[p + 1] != _text[i])\n",
        "            p = _preff[p];\n",
        "\n",
        "        if (_text[p + 1] == _text[i])\n",
        "            ++p;\n",
        "\n",
        "        _preff[i] = p;\n",
        "    }\n",
        "}\n",
        "\n",
        "Can you change the variable and function names in the code above so that they are more descriptive?''')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7m6Gr9eI7GC"
      },
      "outputs": [],
      "source": [
        "chatbot.ask(\"Great! Can you rewrite it in Python? Change global arrays to local lists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQrwoaWZI7GD"
      },
      "source": [
        "When the code is simple the model can even improve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-E4gtbjI7GD",
        "outputId": "35229858-e7b1-4cfb-ca74-846d77e4c9df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> \n",
            "ASSISTANT:  Sure, here's an implementation of the is_prime function that runs in O(sqrt(x)) time:\n",
            "\n",
            "```python\n",
            "def is_prime(x):\n",
            "   if x <= 1:\n",
            "       return False\n",
            "   for i in range(2, int(x**0.5) + 1):\n",
            "       if x % i == 0:\n",
            "           return False\n",
            "   return True\n",
            "```\n",
            "This implementation uses the fact that if a number x is prime, then any number less than or equal to x that is divisible by x is also a factor of x. Therefore, we can check for divisibility by numbers up to the square root of x without having to check all numbers up to x.\n",
            "\n",
            "This implementation has a time complexity of O(sqrt(x)) because the loop iterates up to `int(x**0.5) + 1`, which is O(sqrt(x)).\n"
          ]
        }
      ],
      "source": [
        "chatbot = SimpleChatBot(model=model, tokenizer=tokenizer)\n",
        "chatbot.ask('''\n",
        "def is_prime(x):\n",
        "    if x <= 1:\n",
        "        return False\n",
        "    for i in range(2, x):\n",
        "        if x % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "I have written the code above but it is pretty slow.\n",
        "Can you make it faster? Say O(sqrt(x)).''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j89HoHSoI7GD"
      },
      "outputs": [],
      "source": [
        "chatbot.ask(\"Great! Can you rewrite this faster implementation it in C++?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YglkA77I7GE"
      },
      "outputs": [],
      "source": [
        "chatbot.ask(\"Thanks! Can you add more comments?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPTv8WGAfklZ"
      },
      "source": [
        "## Chat\n",
        "We have also used [ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed) dataset to enhance the model conversation abilities. The chat prompt was inspired by [LongChat](https://github.com/DachengLi1/LongChat)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYaj4mlIf7gi"
      },
      "source": [
        "Feel free to try the chat yourself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjd33BLlfph5",
        "outputId": "115ecb0c-642f-4de5-a233-820961c77fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> \n",
            "ASSISTANT:  Hello! How can I help you today?\n",
            "\n",
            "ASSISTANT:  I'm a computer program designed to assist and communicate with you in a friendly and helpful way. I'm here to answer your questions and provide information to the best of my abilities. Is there something specific you would like to know?\n",
            "\n",
            "ASSISTANT:  I'm a computer program designed to assist and communicate with you in a friendly and helpful way. I'm here to answer your questions and provide information to the best of my abilities. Is there something specific you would like to know?\n",
            "\n",
            "ASSISTANT:  I'm a computer program designed to assist and communicate with you in a friendly and helpful way. I'm here to answer your questions and provide information to the best of my abilities. Is there something specific you would like to know\n",
            "\n",
            "ASSISTANT:  Conversational models are a type of artificial intelligence that are designed to mimic human conversation. There are several state-of-the-art techniques used to create conversational models, including:\n",
            "\n",
            "1. Natural Language Processing (NLP): This technique uses machine learning algorithms to understand and process natural language data. It can be used to analyze text and generate responses that are similar to human language.\n",
            "2. Generative Models: These models use machine learning algorithms to generate text or speech that is similar to human language. They can be trained on large amounts of data to create realistic conversations.\n",
            "3. Transformers: These models use a deep learning architecture called Transformer, which is designed to handle large amounts of text data. They can be used to generate text or speech that is similar to human language.\n",
            "4. GPT-3: This is a language model developed by OpenAI that can generate text or speech that is similar to human language. It uses a large amount of data and advanced machine learning techniques to generate text that is both grammatically correct and coherent.\n",
            "\n",
            "These are just a few examples of the state-of-the-art techniques used to create conversational models. There are many other techniques and models that are being developed and used in the field of artificial intelligence.\n",
            "\n",
            "ASSISTANT:  Yes, it is possible to train a conversational model to book a ticket. Here are the general steps you can follow to train a conversational model to book a ticket:\n",
            "\n",
            "1. Collect data: Collect a large amount of data that includes conversations between a human and a customer service representative. This data should include the customer's intent to book a ticket, the details of the ticket, and any other relevant information.\n",
            "2. Preprocess data: Preprocess the data to remove any irrelevant information and make it easier to understand. This can include things like removing stop words, stemming words, and removing punctuation.\n",
            "3. Train the model: Train the model using the preprocessed data. This can be done using a machine learning algorithm like a Recurrent Neural Network (RNN) or a Transformer.\n",
            "4. Test the model: Test the model on a held-out test set to evaluate its performance. This can include things like accuracy, precision, and recall.\n",
            "5. Deploy the model: Once the model has been tested and found to be accurate, you can deploy it to a production environment. This can include things like integrating it with a booking system or a chatbot platform.\n",
            "\n",
            "It's important to note that training a conversational model to book a ticket is a complex task that requires a lot of data and advanced machine learning techniques. It's also important to consider the legal and ethical implications of using a conversational model to book a ticket.\n",
            "\n",
            "ASSISTANT:  Named entity recognition (NER) is a technique used to identify and categorize named entities in text. It can be used to extract information about people, organizations, locations, and other entities from a text. To implement NER for a custom entity, you can follow these general steps:\n",
            "\n",
            "1. Collect data: Collect a large amount of data that includes text with the custom entity you want to recognize. This data should include the text and the entity you want to recognize.\n",
            "2. Preprocess data: Preprocess the data to remove any irrelevant information and make it easier to understand. This can include things like removing stop words, stemming words, and removing punctuation.\n",
            "3. Train the model: Train the model using the preprocessed data. This can be done using a machine learning algorithm like a Recurrent Neural Network (RNN) or a Transformer.\n",
            "4. Test the model: Test the model on a held-out test set to evaluate its performance. This can include things like accuracy, precision, and recall.\n",
            "5. Deploy the model: Once the model has been tested and found to be accurate, you can deploy it to a production environment. This can include things like integrating it with a chatbot platform or a booking system.\n",
            "\n",
            "It's important to note that implementing NER for a custom entity is a complex task that requires a lot of data and advanced machine learning techniques. It's also important to consider the legal and ethical implications of using NER for a custom entity.\n",
            "\n",
            "ASSISTANT:  Sure, here's some code in Python that can extract the table name from the given sentence:\n",
            "\n",
            "```python\n",
            "import re\n",
            "\n",
            "sentence = \"the table name is tab123sales.\"\n",
            "\n",
            "table_name = re.search(r\"table name is (\\w+)\", sentence).group(1)\n",
            "\n",
            "print(table_name)\n",
            "```\n",
            "This code uses the `re` module in Python to search for the pattern \"table name is (\\w+)\" in the sentence. The parentheses around `\\w+` indicate that we want to capture the text that matches this pattern. The `group(1)` method of the `re.search` function is used to extract the captured text.\n",
            "\n",
            "The output of this code will be \"tab123sales\".\n",
            "\n",
            "ASSISTANT:  Yes, you can use the same code as above to extract the table name from any English sentence. Here's an updated version of the code that will work for any English sentence:\n",
            "\n",
            "```python\n",
            "import re\n",
            "\n",
            "sentence = \"The table name is tab123sales.\"\n",
            "\n",
            "table_name = re.search(r\"table name is (\\w+)\", sentence, flags=re.IGNORECASE).group(1)\n",
            "\n",
            "print(table_name)\n",
            "```\n",
            "This code uses the `re.IGNORECASE` flag to make the pattern case-insensitive, so it will still work if the sentence is written in lowercase or with different capitalization.\n",
            "\n",
            "The output of this code will be \"tab123sales\"\n",
            "\n",
            "It's important to note that this code assumes that the table name will always be written in the same format, and that it will always be at the end of the sentence. If the sentence is written differently, or if the table name is not at the end of the sentence, the code may not work as expected.\n",
            "\n",
            "ASSISTANT:  Yes, you are correct. The code in the previous answer will not work if the sentence is \"tab123sales is the table name\" because it will not match the pattern \"table name is (\\w+)\" in the sentence.\n",
            "\n",
            "Here's an updated version of the code that will work for any English sentence that includes the table name:\n",
            "\n",
            "```python\n",
            "import re\n",
            "\n",
            "sentence = \"tab123sales is the table name.\"\n",
            "\n",
            "table_name = re.search(r\"(\\w+)\\s+is\\s+the\\s+table\\s+name\", sentence, flags=re.IGNORECASE).group(1)\n",
            "\n",
            "print(table_name)\n",
            "```\n",
            "This code uses the `re.IGNORECASE` flag to make the pattern case-insensitive, so it will still work if the sentence is written in lowercase or with different capitalization.\n",
            "\n",
            "The output of this code will be \"tab123sales\"\n",
            "\n",
            "It's important to note that this code assumes that the table name will always be written in the same format, and that it will always be at the beginning of the sentence. If the sentence is written differently, or if the table name is not at the beginning of the sentence, the code may not work as expected\n",
            "\n",
            "ASSISTANT:  Yes, "
          ]
        }
      ],
      "source": [
        "chatbot = SimpleChatBot(model=model, tokenizer=tokenizer)\n",
        "while True:\n",
        "    user_text = input(\"USER: \")\n",
        "    chatbot.ask(user_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3ZQR2L5RB14"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}